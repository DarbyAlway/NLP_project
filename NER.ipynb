{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from TorchCRF import CRF\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample Data (Words and Entity Labels)\n",
    "data = [\n",
    "    ([\"John\", \"lives\", \"in\", \"New\", \"York\"], [\"PER\", \"O\", \"O\", \"LOC\", \"LOC\"]),\n",
    "    ([\"Google\", \"is\", \"a\", \"tech\", \"company\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Elon\", \"Musk\", \"founded\", \"SpaceX\"], [\"PER\", \"PER\", \"O\", \"ORG\"]),\n",
    "    ([\"Microsoft\", \"was\", \"founded\", \"by\", \"Bill\", \"Gates\"], [\"ORG\", \"O\", \"O\", \"O\", \"PER\", \"PER\"]),\n",
    "    ([\"Facebook\", \"headquarters\", \"is\", \"in\", \"California\"], [\"ORG\", \"O\", \"O\", \"O\", \"LOC\"]),\n",
    "    ([\"Jeff\", \"Bezos\", \"started\", \"Amazon\"], [\"PER\", \"PER\", \"O\", \"ORG\"]),\n",
    "    ([\"Paris\", \"is\", \"a\", \"beautiful\", \"city\"], [\"LOC\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Apple\", \"launched\", \"a\", \"new\", \"iPhone\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Tesla\", \"is\", \"leading\", \"the\", \"EV\", \"market\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Mark\", \"Zuckerberg\", \"founded\", \"Meta\"], [\"PER\", \"PER\", \"O\", \"ORG\"]),\n",
    "    ([\"The\", \"Eiffel\", \"Tower\", \"is\", \"in\", \"France\"], [\"O\", \"LOC\", \"LOC\", \"O\", \"O\", \"LOC\"]),\n",
    "    ([\"Cristiano\", \"Ronaldo\", \"plays\", \"for\", \"Al-Nassr\"], [\"PER\", \"PER\", \"O\", \"O\", \"ORG\"]),\n",
    "    ([\"Amazon\", \"is\", \"one\", \"of\", \"the\", \"biggest\", \"companies\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Tokyo\", \"is\", \"a\", \"major\", \"city\", \"in\", \"Japan\"], [\"LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"LOC\"]),\n",
    "    ([\"Sundar\", \"Pichai\", \"is\", \"the\", \"CEO\", \"of\", \"Google\"], [\"PER\", \"PER\", \"O\", \"O\", \"O\", \"O\", \"ORG\"]),\n",
    "    ([\"The\", \"Amazon\", \"River\", \"flows\", \"through\", \"South\", \"America\"], [\"O\", \"LOC\", \"LOC\", \"O\", \"O\", \"LOC\", \"LOC\"]),\n",
    "    ([\"IBM\", \"was\", \"a\", \"leader\", \"in\", \"computing\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"The\", \"Statue\", \"of\", \"Liberty\", \"is\", \"in\", \"New\", \"York\"], [\"O\", \"LOC\", \"O\", \"LOC\", \"O\", \"O\", \"LOC\", \"LOC\"]),\n",
    "    ([\"Taylor\", \"Swift\", \"won\", \"multiple\", \"Grammy\", \"Awards\"], [\"PER\", \"PER\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"The\", \"Great\", \"Wall\", \"of\", \"China\", \"is\", \"a\", \"historic\", \"site\"], [\"O\", \"LOC\", \"LOC\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "]\n",
    "\n",
    "# Mapping Words and Labels to IDs\n",
    "word2idx = {\"<SOS>\": 0, \"<EOS>\": 1, \"UNK\": 2} \n",
    "tag2idx = {\"<SOS>\": 0, \"<EOS>\": 1, \"O\": 2, \"ORG\": 3, \"LOC\": 4,\"UNK\":5}\n",
    "\n",
    "\n",
    "# Indexing words\n",
    "for sentence, tags in data:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "    for tag in tags:\n",
    "        if tag not in tag2idx:\n",
    "            tag2idx[tag] = len(tag2idx)\n",
    "            \n",
    "# Indexing tags\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(model, l1_lambda=0.001):\n",
    "    l1_norm = 0\n",
    "    for param in model.parameters():\n",
    "        l1_norm += param.abs().sum()  # Sum of absolute values of parameters\n",
    "    return l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, mapping):\n",
    "    # Add <SOS> at the start and <EOS> at the end of each sequence\n",
    "    seq = [\"<SOS>\"] + seq + [\"<EOS>\"]\n",
    "    return torch.tensor([mapping.get(w, mapping[\"UNK\"]) for w in seq], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = [(prepare_sequence(sentence, word2idx), prepare_sequence(tags, tag2idx)) for sentence, tags in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "def early_stopping(patience, validation_losses):\n",
    "    if len(validation_losses) < patience:\n",
    "        return False\n",
    "    return np.argmin(validation_losses[-patience:]) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=32, hidden_dim=64, dropout=0.5):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "\n",
    "    def forward(self, sentences, tags=None):\n",
    "        # Embedding and LSTM\n",
    "        embeds = self.embedding(sentences)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        emissions = self.fc(lstm_out)\n",
    "\n",
    "        # If tags are provided, calculate loss (during training)\n",
    "        if tags is not None:\n",
    "            # Masking out the <SOS> and <EOS> positions from the loss calculation\n",
    "            mask = sentences != 0  # Mask where sentences are not padded\n",
    "            mask &= sentences != 1  # Also mask out the <EOS> token (index 1)\n",
    "\n",
    "            loss = -self.crf(emissions, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            # During inference, return the best tag sequence (Viterbi decoding)\n",
    "            mask = sentences != 0  # Mask out padding\n",
    "            return self.crf.viterbi_decode(emissions, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 254.8513, Accuracy: 0.2564\n",
      "Epoch 20, Loss: -38.6287, Accuracy: 0.7778\n",
      "Epoch 40, Loss: -265.2643, Accuracy: 0.8120\n",
      "Epoch 60, Loss: -513.0708, Accuracy: 0.8291\n",
      "Epoch 80, Loss: -782.4031, Accuracy: 0.8291\n",
      "Epoch 100, Loss: -1018.9456, Accuracy: 0.8291\n",
      "Epoch 120, Loss: -1257.9093, Accuracy: 0.8291\n",
      "Epoch 140, Loss: -1474.0025, Accuracy: 0.8291\n",
      "Epoch 160, Loss: -1734.5525, Accuracy: 0.8205\n",
      "Epoch 180, Loss: -1932.1201, Accuracy: 0.8291\n",
      "Epoch 200, Loss: -2160.5891, Accuracy: 0.8291\n",
      "Epoch 220, Loss: -2527.5564, Accuracy: 0.8291\n",
      "Epoch 240, Loss: -2710.4619, Accuracy: 0.8291\n",
      "Epoch 260, Loss: -2946.6532, Accuracy: 0.8291\n",
      "Epoch 280, Loss: -2997.5664, Accuracy: 0.8291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Model, Optimizer, and Loss\n",
    "model = BiLSTM_CRF(len(word2idx), len(tag2idx))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001) \n",
    "\n",
    "patience = 5 \n",
    "validation_losses = []  \n",
    "# Training Loop\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for words, tags in train_data:\n",
    "        words, tags = words.unsqueeze(0), tags.unsqueeze(0)  # Add batch dimension\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(words, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Prediction step\n",
    "        predicted_tags = model(words)\n",
    "        predicted_tags = predicted_tags[0]  # Get the first sentence's predictions\n",
    "        \n",
    "        # Exclude SOS and EOS from accuracy calculation\n",
    "        predicted_tags = predicted_tags[1:-1]  # Remove <SOS> and <EOS> tokens\n",
    "        tags = tags.squeeze(0)[1:-1]  # Remove <SOS> and <EOS> tokens\n",
    "\n",
    "        correct_predictions += sum(p == t for p, t in zip(predicted_tags, tags))    \n",
    "        total_predictions += tags.nelement()  # Total number of tags\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entity Recognition: Prediction Results:\n",
      "Apple: LOC\n",
      "is: ORG\n",
      "a: O\n",
      "tech: O\n",
      "company: O\n",
      "\n",
      "Named Entity Recognition: Prediction Results:\n",
      "Barack: PER\n",
      "Obama: PER\n",
      "was: O\n",
      "born: O\n",
      "in: O\n",
      "Hawaii: O\n",
      "\n",
      "Named Entity Recognition: Prediction Results:\n",
      "Microsoft: LOC\n",
      "acquired: ORG\n",
      "LinkedIn: O\n",
      "\n",
      "Accuracy: 0.7857\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    ([\"Apple\", \"is\", \"a\", \"tech\", \"company\"], [\"ORG\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    ([\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"], [\"PER\", \"PER\", \"O\", \"O\", \"O\", \"LOC\"]),\n",
    "    ([\"Microsoft\", \"acquired\", \"LinkedIn\"], [\"ORG\", \"O\", \"ORG\"]),\n",
    "]\n",
    "\n",
    "# Convert Test Data to Index Form\n",
    "test_data_idx = [(prepare_sequence(sentence, word2idx), prepare_sequence(tags, tag2idx)) for sentence, tags in test_data]\n",
    "\n",
    "# Prepare for Inference on Test Data\n",
    "model.eval()  # Switch the model to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Inference on Test Data (without tags, as we're predicting)\n",
    "for sentence, true_tags in test_data:\n",
    "    # Convert the sentence to indices\n",
    "    test_seq = prepare_sequence(sentence, word2idx).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predicted_tags = model(test_seq)  # Get the predicted tag sequence\n",
    "    \n",
    "    # Convert predicted tags to their corresponding string labels\n",
    "    predicted_tags = predicted_tags[0]  # Extract the predicted tags for this sentence\n",
    "    predicted_tag_labels = [idx2tag[tag] for tag in predicted_tags]\n",
    "\n",
    "    # Print the results for the entire sentence\n",
    "    print(\"\\nNamed Entity Recognition: Prediction Results:\")\n",
    "    for word, predicted_tag in zip(sentence, predicted_tag_labels):\n",
    "        print(f\"{word}: {predicted_tag}\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    true_tags_idx = prepare_sequence(true_tags, tag2idx)\n",
    "    correct += sum(p == t for p, t in zip(predicted_tags, true_tags_idx))\n",
    "    total += len(true_tags)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
